{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import các thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer \n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Định nghĩa các biến"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "white_space_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ID_OF_DOC_FOR_QUERY = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Các hàm thành phần"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getFilePathList(path):\n",
    "    list_file_path = list()\n",
    "    \n",
    "    for file_name in os.listdir(path):\n",
    "        full_file_path = path + \"\\\\\" + file_name\n",
    "        \n",
    "        if os.path.isfile(full_file_path):\n",
    "            list_file_path.append(full_file_path)\n",
    "            \n",
    "    return list_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def indexingDocument(list_file_path):\n",
    "    id_path_of_files = dict()\n",
    "    \n",
    "    for index, file_path in enumerate(list_file_path):\n",
    "        id_path_of_files[index] = file_path\n",
    "    \n",
    "    return id_path_of_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessString(string_data, mode_of_preprocessing):\n",
    "    # Lower the text\n",
    "    preprocess_data = string_data.lower()\n",
    "    \n",
    "    # Remove Unicode characters\n",
    "    preprocess_data = preprocess_data.encode('ascii', 'ignore').decode()\n",
    "    \n",
    "    # One letter in a word should not be present more than twice in continuation, ex: \"I misssss youuu\" -> \"I miss youu\"\n",
    "    preprocess_data = ''.join(''.join(s)[:2] for _, s in itertools.groupby(preprocess_data)) \n",
    "    \n",
    "    # Remove punctuations, each punctuation = space, ex: \"\"information @#$retrieval\" -> \"information    retrieval\"\n",
    "    preprocess_data = re.sub('[%s]' % re.escape(string.punctuation), ' ', preprocess_data)   \n",
    "        \n",
    "    # Tokenize word by white space\n",
    "    preprocess_data = white_space_tokenizer.tokenize(preprocess_data)\n",
    "    \n",
    "############################################################################################   \n",
    "    \n",
    "    \n",
    "    # Remove stop words\n",
    "#     preprocess_data = [word for word in preprocess_data if word not in stop_words]\n",
    "        \n",
    "    # Stem word\n",
    "#     preprocess_data = [porter_stemmer.stem(word) for word in preprocess_data]\n",
    "  \n",
    "    # Lemmatize word\n",
    "#     preprocess_data = [wordnet_lemmatizer.lemmatize(word) for word in preprocess_data]\n",
    "\n",
    "    if mode_of_preprocessing == 'stopWord-lemmatize':\n",
    "        # Remove stop words\n",
    "        preprocess_data = [word for word in preprocess_data if word not in stop_words]\n",
    "        # Lemmatize word\n",
    "        preprocess_data = [wordnet_lemmatizer.lemmatize(word) for word in preprocess_data]\n",
    "        \n",
    "    elif mode_of_preprocessing == 'stopWord-stem':\n",
    "        # Remove stop words\n",
    "        preprocess_data = [word for word in preprocess_data if word not in stop_words]\n",
    "        # Stem word\n",
    "        preprocess_data = [porter_stemmer.stem(word) for word in preprocess_data]\n",
    "        \n",
    "    elif mode_of_preprocessing == 'stopWord-noStem':\n",
    "        # Remove stop words\n",
    "        preprocess_data = [word for word in preprocess_data if word not in stop_words]\n",
    "        \n",
    "    elif mode_of_preprocessing == 'noStopWord-lemmatize':\n",
    "        # Lemmatize word\n",
    "        preprocess_data = [wordnet_lemmatizer.lemmatize(word) for word in preprocess_data]\n",
    "        \n",
    "    elif mode_of_preprocessing == 'noStopWord-stem':\n",
    "        # Stem word\n",
    "        preprocess_data = [porter_stemmer.stem(word) for word in preprocess_data]\n",
    "        \n",
    "    elif mode_of_preprocessing == 'noStopWord-noStem':\n",
    "        pass\n",
    "        \n",
    "\n",
    "############################################################################################\n",
    "        \n",
    "    return preprocess_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'have', 'i', 'done', 'and', 'why', 'who', 's', 'this']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_a = \"What have I done? And why? Who's this\"\n",
    "# test_a = preprocessString(test_a, 'noStopWord-noStem')\n",
    "# test_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['done']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_b = \"What have I done\"\n",
    "# test_b = preprocessString(test_b, 'stopWord-noStem')\n",
    "# test_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_termID_forQuery(dictionary_of_docs, query, mode_of_preprocessing):\n",
    "    term_id = list()\n",
    "    \n",
    "    query = preprocessString(query, mode_of_preprocessing)\n",
    "    \n",
    "    for term in query:\n",
    "        if term in dictionary_of_docs.keys():\n",
    "            term_id.append([term, ID_OF_DOC_FOR_QUERY])\n",
    "    \n",
    "    return term_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_termID_forDocument(id_path_of_files, mode_of_preprocessing):\n",
    "    term_id = list()\n",
    "    \n",
    "    for index, file_path in id_path_of_files.items():\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.readlines()\n",
    "            f.close()\n",
    "            \n",
    "            lyric = str()\n",
    "            for i in range(2, len(content)):\n",
    "                lyric = lyric + ' ' + content[i]           \n",
    "            lyric = preprocessString(lyric, mode_of_preprocessing)\n",
    "            \n",
    "            for term in lyric:\n",
    "                term_id.append([term, index])\n",
    "    \n",
    "    return term_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createDictionaryAndVectorDoc(term_id):\n",
    "    dictionary = dict()\n",
    "    vector_docs = dict()\n",
    "        \n",
    "    for term, id_of_doc in term_id:\n",
    "        \n",
    "        if id_of_doc not in vector_docs.keys():\n",
    "            vector_docs[id_of_doc] = {term}\n",
    "            \n",
    "        elif term not in vector_docs[id_of_doc]:\n",
    "            vector_docs[id_of_doc].add(term)\n",
    "        \n",
    "        \n",
    "        # Nếu term chưa có trong dictionary thì thêm term, ndoc, id_tf vào\n",
    "        if term not in dictionary.keys():\n",
    "            dictionary[term] = {'ndoc': 1,\n",
    "                                'id_tf': {id_of_doc: 1}}\n",
    "            \n",
    "        # Nếu term đã có trong dictionary rồi thì sẽ cập nhật các chỉ số ndoc, id_tf nếu thỏa điều kiện\n",
    "        else:\n",
    "            \n",
    "            # Nếu term này đã xuất hiện trong id_tf thì chỉ cập nhật mỗi id_tf\n",
    "            # (tức là cập nhật tần số xuất hiện của term trong document này)\n",
    "            if id_of_doc in dictionary[term]['id_tf'].keys():\n",
    "                dictionary[term]['id_tf'][id_of_doc] += 1\n",
    "                \n",
    "            # Nếu term này chưa xuất hiện trong id_tf thì phải cập nhật thêm ndoc lên 1 đơn vị\n",
    "            # và cập nhật thêm một cặp id_tf mới cho term\n",
    "            else:\n",
    "                dictionary[term]['ndoc'] += 1\n",
    "                dictionary[term]['id_tf'][id_of_doc] = 1\n",
    "            \n",
    "    return [dictionary, vector_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay giá trị tf bằng weight tf: 1 + math.log(tf, 10)\n",
    "\n",
    "def calculateTF(dictionary):\n",
    "    for term in dictionary.keys():        \n",
    "        for doc_id in dictionary[term]['id_tf'].keys():\n",
    "            tf = dictionary[term]['id_tf'][doc_id]\n",
    "            dictionary[term]['id_tf'][doc_id] = 1 + math.log(tf, 10)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay giá trị ndoc bằng IDF = log(number_of_docs / ndoc, 10)\n",
    "\n",
    "def calculateIDF(dictionary, number_of_docs):\n",
    "    for term in dictionary.keys():\n",
    "        ndoc = dictionary[term]['ndoc']\n",
    "        dictionary[term]['ndoc'] = math.log(number_of_docs / ndoc, 10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculateIDF_forQuery(dictionary_of_docs, dictionary_of_query):\n",
    "    for term in dictionary_of_query.keys():\n",
    "        ndoc = dictionary_of_docs[term]['ndoc']\n",
    "        dictionary_of_query[term]['ndoc'] = ndoc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay giá trị id_tf bằng tf-idf = tf * idf\n",
    "\n",
    "def calculate_TF_IDF(dictionary):\n",
    "    for term in dictionary.keys():\n",
    "        idf_of_term = dictionary[term]['ndoc']\n",
    "        \n",
    "        for id_of_doc in dictionary[term]['id_tf'].keys():\n",
    "            tf_of_term_in_doc = dictionary[term]['id_tf'][id_of_doc]\n",
    "                      \n",
    "            dictionary[term]['id_tf'][id_of_doc] = tf_of_term_in_doc * idf_of_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector là list\n",
    "\n",
    "def calculateDenominatorOfVector(vector):\n",
    "    denominator = 0\n",
    "    \n",
    "    for value in vector:\n",
    "        denominator += math.pow(value, 2)\n",
    "        \n",
    "    sqrt_denominator = math.sqrt(denominator)\n",
    "    \n",
    "    if sqrt_denominator > 0:\n",
    "        return sqrt_denominator\n",
    "    else:\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalizeDictionary(dictionary, vector_docs):\n",
    "    \n",
    "    for id_of_doc in vector_docs.keys():\n",
    "        vector_weight_of_document = list()\n",
    "        \n",
    "        for term in vector_docs[id_of_doc]:\n",
    "            tf_idf_of_term = dictionary[term]['id_tf'][id_of_doc]\n",
    "            vector_weight_of_document.append(tf_idf_of_term)\n",
    "            \n",
    "        denominator_of_vector = calculateDenominatorOfVector(vector_weight_of_document)\n",
    "        \n",
    "        for term in vector_docs[id_of_doc]:\n",
    "            tf_idf_of_term = dictionary[term]['id_tf'][id_of_doc]\n",
    "            dictionary[term]['id_tf'][id_of_doc] = tf_idf_of_term / denominator_of_vector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Union(set_of_doc, set_of_query):    \n",
    "    union_set = set().union(set_of_doc, set_of_query)\n",
    "    return union_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processDocumentsFromFoler(path, mode_of_preprocessing):\n",
    "    \n",
    "    list_file_path = getFilePathList(path)\n",
    "\n",
    "    id_path_of_files = indexingDocument(list_file_path)\n",
    "\n",
    "    term_id = create_termID_forDocument(id_path_of_files, mode_of_preprocessing)\n",
    "\n",
    "    dictionary, vector_docs = createDictionaryAndVectorDoc(term_id)\n",
    "\n",
    "    calculateTF(dictionary)\n",
    "\n",
    "    calculateIDF(dictionary, len(id_path_of_files))\n",
    "\n",
    "    calculate_TF_IDF(dictionary)\n",
    "\n",
    "    normalizeDictionary(dictionary, vector_docs)\n",
    "    \n",
    "    return [dictionary, vector_docs, id_path_of_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processQuery(dictionary_of_docs, query, mode_of_preprocessing):\n",
    "    \n",
    "    term_id = create_termID_forQuery(dictionary_of_docs, query, mode_of_preprocessing)\n",
    "    \n",
    "    dictionary_of_query, vector_docs = createDictionaryAndVectorDoc(term_id)\n",
    "\n",
    "    calculateTF(dictionary_of_query)\n",
    "\n",
    "    calculateIDF_forQuery(dictionary_of_docs, dictionary_of_query)\n",
    "\n",
    "    calculate_TF_IDF(dictionary_of_query)\n",
    "\n",
    "    normalizeDictionary(dictionary_of_query, vector_docs)\n",
    "    \n",
    "    return dictionary_of_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculateCosineSimilarity(dictionary_of_docs, dictionary_of_query):\n",
    "    \n",
    "    weight_of_documents = dict()\n",
    "    \n",
    "    for term in dictionary_of_query.keys():\n",
    "        weight_of_term_in_query = dictionary_of_query[term]['id_tf'][ID_OF_DOC_FOR_QUERY]      \n",
    "        \n",
    "        for id_of_doc in dictionary_of_docs[term]['id_tf'].keys():\n",
    "            \n",
    "            weight_of_term_in_document =  dictionary_of_docs[term]['id_tf'][id_of_doc]\n",
    "            \n",
    "            if id_of_doc not in weight_of_documents.keys():\n",
    "                weight_of_documents[id_of_doc] = weight_of_term_in_document * weight_of_term_in_query\n",
    "            else:\n",
    "                weight_of_documents[id_of_doc] += weight_of_term_in_document * weight_of_term_in_query\n",
    "          \n",
    "    \n",
    "    sorted_weight_of_documents = sorted(weight_of_documents.items(),\n",
    "                                        key = operator.itemgetter(1),\n",
    "                                        reverse = True)\n",
    "    return sorted_weight_of_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculateEuclidSimilarity(dictionary_of_docs, dictionary_of_query, vector_docs):\n",
    "    \n",
    "    weight_of_documents = dict()\n",
    "    \n",
    "    set_of_query = set(dictionary_of_query.keys())\n",
    "    \n",
    "    for id_of_doc in vector_docs.keys():\n",
    "        union_term = Union(vector_docs[id_of_doc], set_of_query)\n",
    "        \n",
    "        for term in union_term:\n",
    "            \n",
    "            # weight_of_term_in_document\n",
    "            if term in vector_docs[id_of_doc]:\n",
    "                weight_of_term_in_document = dictionary_of_docs[term]['id_tf'][id_of_doc]\n",
    "            else:\n",
    "                weight_of_term_in_document = 0\n",
    "            \n",
    "            # weight_of_term_in_query\n",
    "            if term in dictionary_of_query.keys():\n",
    "                weight_of_term_in_query = dictionary_of_query[term]['id_tf'][ID_OF_DOC_FOR_QUERY]\n",
    "            else:\n",
    "                weight_of_term_in_query = 0\n",
    "                \n",
    "            if id_of_doc not in weight_of_documents.keys():\n",
    "                weight_of_documents[id_of_doc] = math.pow(weight_of_term_in_document - weight_of_term_in_query, 2)\n",
    "            else:\n",
    "                weight_of_documents[id_of_doc] += math.pow(weight_of_term_in_document - weight_of_term_in_query, 2)\n",
    "    \n",
    "    for id_of_doc in weight_of_documents.keys():\n",
    "        value = weight_of_documents[id_of_doc]\n",
    "        weight_of_documents[id_of_doc] = math.sqrt(value)\n",
    "    \n",
    "    sorted_weight_of_documents = sorted(weight_of_documents.items(),\n",
    "                                        key=operator.itemgetter(1),\n",
    "                                        reverse = False)\n",
    "    return sorted_weight_of_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_topK_result(sorted_weight_of_documents,\n",
    "                      id_path_of_files,\n",
    "                      top_k_results_to_return,\n",
    "                      similarity_measure):\n",
    "    \n",
    "    length_of_docs_returned = len(sorted_weight_of_documents)\n",
    "    \n",
    "    # Đối với code này:\n",
    "    # Euclid sẽ không bao giờ xảy ra trường hợp không có kết quả trả về, những kết quả top-k có thể có những giá trị sqrt(2)\n",
    "    # Còn Cosine thì vẫn có trường hợp không có kết quả trả về, tại vì xét trên những term trong query\n",
    "    if length_of_docs_returned == 0:\n",
    "        print(\"\\nNo such song relate to query!!!\\n\")\n",
    "        return -1\n",
    "    \n",
    "    if similarity_measure == 'Cosine':\n",
    "        print(\"\\n--------------- Higher Score Is Better ---------------\\n\")\n",
    "    elif similarity_measure == 'Euclid':\n",
    "        print(\"\\n--------------- Lower Score Is Better ---------------\\n\")\n",
    "    \n",
    "    if top_k_results_to_return > length_of_docs_returned:\n",
    "        top_k_results_to_return = length_of_docs_returned\n",
    "        print(\"Only \" + str(top_k_results_to_return) + \" relate to query\\n\")\n",
    "        \n",
    "    for index in range(top_k_results_to_return):\n",
    "        score = sorted_weight_of_documents[index][1]\n",
    "        print('Top', index + 1, ':', score)\n",
    "        \n",
    "    print('\\n')\n",
    "    \n",
    "    for index in range(top_k_results_to_return):\n",
    "        file_id = sorted_weight_of_documents[index][0]\n",
    "        file_path = id_path_of_files[file_id]\n",
    "        score = sorted_weight_of_documents[index][1]\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read().strip().split('\\n')\n",
    "            file.close()\n",
    "            print('Position ' + str(index + 1) + ':')\n",
    "            print('Song:', content[0])\n",
    "            print('Artist:', content[1])\n",
    "            print('Lyric:', content[2])\n",
    "            print('\\n')      \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_Vector_Space_Model(path, mode_of_preprocessing):\n",
    "    return processDocumentsFromFoler(path, mode_of_preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def searchDocumentWithQuery(dictionary_of_docs,\n",
    "                            vector_docs,\n",
    "                            id_path_of_files,\n",
    "                            query,\n",
    "                            top_k_results_to_return,\n",
    "                            mode_of_preprocessing,\n",
    "                            similarity_measure):\n",
    "    \n",
    "    dictionary_of_query = processQuery(dictionary_of_docs, query, mode_of_preprocessing)\n",
    "    \n",
    "    sorted_weight_of_documents = list()\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    \n",
    "    if similarity_measure == 'Cosine':\n",
    "        sorted_weight_of_documents = calculateCosineSimilarity(dictionary_of_docs, dictionary_of_query)\n",
    "    elif similarity_measure == 'Euclid':\n",
    "        sorted_weight_of_documents = calculateEuclidSimilarity(dictionary_of_docs, dictionary_of_query, vector_docs)\n",
    "        \n",
    "    end_time = time.process_time()\n",
    "    \n",
    "    print('\\nTime To Search: ', end_time - start_time, \" seconds\")\n",
    "        \n",
    "    print_topK_result(sorted_weight_of_documents, id_path_of_files, top_k_results_to_return, similarity_measure)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đường dẫn tới những file txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_of_docs = 'D:\\\\A_Truy_Van_Thong_Tin_Da_Phuong_Tien\\\\song_spotify\\\\8982_txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Stopword removal options:\n",
      "\t 1. Stopword removal using NLTK stopword list\n",
      "\t 2. No stopword removal\n",
      "Please choose stopword removal option: 1\n",
      "\n",
      "* Word stemming options:\n",
      "\t 1. WordNet Lemmatizer\n",
      "\t 2. Porter Stemmer\n",
      "\t 3. No word stemming\n",
      "Please choose word stemming option: 1\n",
      "\n",
      "Method applied:\n",
      "\t - Stopword removal: Stopword removal using NLTK stopword list\n",
      "\t - Word stemming: WordNet Lemmatizer\n",
      "\t - Term weighting: TF-IDF\n",
      "\n",
      "\n",
      "--------------------------- Initializing Vector Space Model ---------------------------\n",
      "\n",
      "Initializing Finished\n",
      "Time To Build Model:  28.515625  seconds\n",
      "Ready To Search\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n* Stopword removal options:')\n",
    "print('\\t 1. Stopword removal using NLTK stopword list')\n",
    "print('\\t 2. No stopword removal')\n",
    "stopword_removal_option = int(input('Please choose stopword removal option: '))\n",
    "\n",
    "while(stopword_removal_option not in [1, 2]):\n",
    "    stopword_removal_option = int(input('Please choose stopword removal option again: '))\n",
    "    \n",
    "mode = str()\n",
    "if stopword_removal_option == 1:\n",
    "    mode += 'stopWord-'\n",
    "elif stopword_removal_option == 2:\n",
    "    mode += 'noStopWord-'\n",
    "    \n",
    "print('\\n* Word stemming options:')\n",
    "print('\\t 1. WordNet Lemmatizer')\n",
    "print('\\t 2. Porter Stemmer')\n",
    "print('\\t 3. No word stemming')\n",
    "word_stemming_option = int(input('Please choose word stemming option: '))\n",
    "\n",
    "while(word_stemming_option not in [1, 2, 3]):\n",
    "    word_stemming_option = int(input('Please choose word stemming option again: '))\n",
    "\n",
    "if word_stemming_option == 1:\n",
    "    mode += 'lemmatize'\n",
    "elif word_stemming_option == 2:\n",
    "    mode += 'stem'\n",
    "elif word_stemming_option == 3:\n",
    "    mode += 'noStem'\n",
    "  \n",
    "\n",
    "\n",
    "print('\\nMethod applied:')\n",
    "# Liệt kê các thông tin của phương pháp được áp dụng\n",
    "\n",
    "if stopword_removal_option == 1:   \n",
    "    print('\\t - Stopword removal: Stopword removal using NLTK stopword list')\n",
    "elif stopword_removal_option == 2:\n",
    "    print('\\t - No stopword removal')\n",
    "    \n",
    "if word_stemming_option == 1:\n",
    "    print('\\t - Word stemming: WordNet Lemmatizer')\n",
    "elif word_stemming_option == 2:\n",
    "    print('\\t - Word stemming: Porter Stemmer')\n",
    "elif word_stemming_option == 3:\n",
    "    print('\\t - No word stemming')\n",
    "\n",
    "print('\\t - Term weighting: TF-IDF')\n",
    "\n",
    "    \n",
    "print('\\n\\n--------------------------- Initializing Vector Space Model ---------------------------\\n')\n",
    "\n",
    "start_time = time.process_time()\n",
    "dict_of_docs, vector_of_docs, id_files = init_Vector_Space_Model(file_path_of_docs, mode)\n",
    "end_time = time.process_time()\n",
    "\n",
    "print('Initializing Finished')\n",
    "print('Time To Build Model: ', end_time - start_time, \" seconds\")\n",
    "print('Ready To Search')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In ra những term nằm trong tất cả document\n",
    "\n",
    "# for term in dict_of_docs.keys():\n",
    "#     if dict_of_docs[term]['ndoc'] == 0:\n",
    "#         print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Similarity measure options:\n",
      "\t 1. Cosine Similarity\n",
      "\t 2. Eulidean Distance\n",
      "Please choose Similarity Measure option: 1\n",
      "\t - Similarity Measure: Cosine Similarity\n",
      "\n",
      "Please type query to search: vector space model\n",
      "\n",
      "Please choose top k results to return: 1\n",
      "\n",
      "Time To Search:  0.0  seconds\n",
      "\n",
      "--------------- Higher Score Is Better ---------------\n",
      "\n",
      "Top 1 : 0.32690320509635107\n",
      "\n",
      "\n",
      "Position 1:\n",
      "Song: Trouble - Jengi Beats Remix\n",
      "Artist: OFFAIAH\n",
      "Lyric: I'm facing the bottle For all of my problems These Instagram models Are nothing but trouble I'm facing the bottle For all of my problems These Instagram models Are nothing but trouble I'm facing the bottle For all of my problems These Instagram models Are nothing but trouble I'm facing the bottle For all of my problems These Instagram models Are nothing but trouble I'm facing the bottle For all of my problems These Instagram models Are nothing but trouble I'm facing the bottle For all of my problems These Instagram models Are nothing but trouble I'm facing the bottle For all of my problems These Instagram models Are nothing but trouble I'm facing the battle Through all of my problems These Instagram models Are nothing but trouble I'm facing the battle Through all of my problems These Instagram models Are nothing but trouble I'm facing the battle Through all of my problems These Instagram models Are nothing but trouble I'm facing the battle Through all of my problems These Instagram models Are nothing but trouble I'm facing the battle Through all of my problems These Instagram models Are nothing but trouble\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n* Similarity measure options:')\n",
    "print('\\t 1. Cosine Similarity')\n",
    "print('\\t 2. Eulidean Distance')\n",
    "ranking_function_option = int(input('Please choose Similarity Measure option: '))\n",
    "simi_measure = str()\n",
    "\n",
    "while(ranking_function_option not in [1, 2]):\n",
    "    ranking_function_option = int(input('Please choose Similarity measure option again: '))\n",
    "\n",
    "if ranking_function_option == 1:\n",
    "    print('\\t - Similarity Measure: Cosine Similarity')\n",
    "    simi_measure = 'Cosine'\n",
    "elif ranking_function_option == 2:\n",
    "    print('\\t - Similarity Measuren: Eclidean Distance')\n",
    "    simi_measure = 'Euclid'\n",
    "\n",
    "\n",
    "query_to_search = input('\\nPlease type query to search: ')\n",
    "top_k = int(input('\\nPlease choose top k results to return: '))\n",
    "searchDocumentWithQuery(dict_of_docs, vector_of_docs, id_files, query_to_search, top_k, mode, simi_measure)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
